{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Allstate Claims Severity.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JydmK3lymK-5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ8Bfaw4mV-9",
        "outputId": "6057e172-44c9-4e2b-b4ee-a4d4b69d3247"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlrwYdE3mQRh"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras.layers import Embedding, Dense, Input, MaxPooling1D, concatenate, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import LSTM, Bidirectional, TimeDistributed\n",
        "from keras.models import Model"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "Y37hcopQmQTw",
        "outputId": "4baa6436-1ea3-4018-c4d8-57bcf66c39ff"
      },
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/Allstate Claims Severity/train.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/Allstate Claims Severity/test.csv')\n",
        "train_df.head(5)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>cat1</th>\n",
              "      <th>cat2</th>\n",
              "      <th>cat3</th>\n",
              "      <th>cat4</th>\n",
              "      <th>cat5</th>\n",
              "      <th>cat6</th>\n",
              "      <th>cat7</th>\n",
              "      <th>cat8</th>\n",
              "      <th>cat9</th>\n",
              "      <th>cat10</th>\n",
              "      <th>cat11</th>\n",
              "      <th>cat12</th>\n",
              "      <th>cat13</th>\n",
              "      <th>cat14</th>\n",
              "      <th>cat15</th>\n",
              "      <th>cat16</th>\n",
              "      <th>cat17</th>\n",
              "      <th>cat18</th>\n",
              "      <th>cat19</th>\n",
              "      <th>cat20</th>\n",
              "      <th>cat21</th>\n",
              "      <th>cat22</th>\n",
              "      <th>cat23</th>\n",
              "      <th>cat24</th>\n",
              "      <th>cat25</th>\n",
              "      <th>cat26</th>\n",
              "      <th>cat27</th>\n",
              "      <th>cat28</th>\n",
              "      <th>cat29</th>\n",
              "      <th>cat30</th>\n",
              "      <th>cat31</th>\n",
              "      <th>cat32</th>\n",
              "      <th>cat33</th>\n",
              "      <th>cat34</th>\n",
              "      <th>cat35</th>\n",
              "      <th>cat36</th>\n",
              "      <th>cat37</th>\n",
              "      <th>cat38</th>\n",
              "      <th>cat39</th>\n",
              "      <th>...</th>\n",
              "      <th>cat92</th>\n",
              "      <th>cat93</th>\n",
              "      <th>cat94</th>\n",
              "      <th>cat95</th>\n",
              "      <th>cat96</th>\n",
              "      <th>cat97</th>\n",
              "      <th>cat98</th>\n",
              "      <th>cat99</th>\n",
              "      <th>cat100</th>\n",
              "      <th>cat101</th>\n",
              "      <th>cat102</th>\n",
              "      <th>cat103</th>\n",
              "      <th>cat104</th>\n",
              "      <th>cat105</th>\n",
              "      <th>cat106</th>\n",
              "      <th>cat107</th>\n",
              "      <th>cat108</th>\n",
              "      <th>cat109</th>\n",
              "      <th>cat110</th>\n",
              "      <th>cat111</th>\n",
              "      <th>cat112</th>\n",
              "      <th>cat113</th>\n",
              "      <th>cat114</th>\n",
              "      <th>cat115</th>\n",
              "      <th>cat116</th>\n",
              "      <th>cont1</th>\n",
              "      <th>cont2</th>\n",
              "      <th>cont3</th>\n",
              "      <th>cont4</th>\n",
              "      <th>cont5</th>\n",
              "      <th>cont6</th>\n",
              "      <th>cont7</th>\n",
              "      <th>cont8</th>\n",
              "      <th>cont9</th>\n",
              "      <th>cont10</th>\n",
              "      <th>cont11</th>\n",
              "      <th>cont12</th>\n",
              "      <th>cont13</th>\n",
              "      <th>cont14</th>\n",
              "      <th>loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>...</td>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "      <td>B</td>\n",
              "      <td>C</td>\n",
              "      <td>E</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>T</td>\n",
              "      <td>B</td>\n",
              "      <td>G</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>I</td>\n",
              "      <td>E</td>\n",
              "      <td>G</td>\n",
              "      <td>J</td>\n",
              "      <td>G</td>\n",
              "      <td>BU</td>\n",
              "      <td>BC</td>\n",
              "      <td>C</td>\n",
              "      <td>AS</td>\n",
              "      <td>S</td>\n",
              "      <td>A</td>\n",
              "      <td>O</td>\n",
              "      <td>LB</td>\n",
              "      <td>0.726300</td>\n",
              "      <td>0.245921</td>\n",
              "      <td>0.187583</td>\n",
              "      <td>0.789639</td>\n",
              "      <td>0.310061</td>\n",
              "      <td>0.718367</td>\n",
              "      <td>0.335060</td>\n",
              "      <td>0.30260</td>\n",
              "      <td>0.67135</td>\n",
              "      <td>0.83510</td>\n",
              "      <td>0.569745</td>\n",
              "      <td>0.594646</td>\n",
              "      <td>0.822493</td>\n",
              "      <td>0.714843</td>\n",
              "      <td>2213.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>...</td>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "      <td>C</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>D</td>\n",
              "      <td>T</td>\n",
              "      <td>L</td>\n",
              "      <td>F</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>I</td>\n",
              "      <td>K</td>\n",
              "      <td>K</td>\n",
              "      <td>BI</td>\n",
              "      <td>CQ</td>\n",
              "      <td>A</td>\n",
              "      <td>AV</td>\n",
              "      <td>BM</td>\n",
              "      <td>A</td>\n",
              "      <td>O</td>\n",
              "      <td>DP</td>\n",
              "      <td>0.330514</td>\n",
              "      <td>0.737068</td>\n",
              "      <td>0.592681</td>\n",
              "      <td>0.614134</td>\n",
              "      <td>0.885834</td>\n",
              "      <td>0.438917</td>\n",
              "      <td>0.436585</td>\n",
              "      <td>0.60087</td>\n",
              "      <td>0.35127</td>\n",
              "      <td>0.43919</td>\n",
              "      <td>0.338312</td>\n",
              "      <td>0.366307</td>\n",
              "      <td>0.611431</td>\n",
              "      <td>0.304496</td>\n",
              "      <td>1283.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>...</td>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "      <td>C</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "      <td>L</td>\n",
              "      <td>O</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>E</td>\n",
              "      <td>F</td>\n",
              "      <td>H</td>\n",
              "      <td>F</td>\n",
              "      <td>A</td>\n",
              "      <td>AB</td>\n",
              "      <td>DK</td>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "      <td>AF</td>\n",
              "      <td>A</td>\n",
              "      <td>I</td>\n",
              "      <td>GK</td>\n",
              "      <td>0.261841</td>\n",
              "      <td>0.358319</td>\n",
              "      <td>0.484196</td>\n",
              "      <td>0.236924</td>\n",
              "      <td>0.397069</td>\n",
              "      <td>0.289648</td>\n",
              "      <td>0.315545</td>\n",
              "      <td>0.27320</td>\n",
              "      <td>0.26076</td>\n",
              "      <td>0.32446</td>\n",
              "      <td>0.381398</td>\n",
              "      <td>0.373424</td>\n",
              "      <td>0.195709</td>\n",
              "      <td>0.774425</td>\n",
              "      <td>3005.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>...</td>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "      <td>C</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>D</td>\n",
              "      <td>T</td>\n",
              "      <td>I</td>\n",
              "      <td>D</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>I</td>\n",
              "      <td>K</td>\n",
              "      <td>K</td>\n",
              "      <td>BI</td>\n",
              "      <td>CS</td>\n",
              "      <td>C</td>\n",
              "      <td>N</td>\n",
              "      <td>AE</td>\n",
              "      <td>A</td>\n",
              "      <td>O</td>\n",
              "      <td>DJ</td>\n",
              "      <td>0.321594</td>\n",
              "      <td>0.555782</td>\n",
              "      <td>0.527991</td>\n",
              "      <td>0.373816</td>\n",
              "      <td>0.422268</td>\n",
              "      <td>0.440945</td>\n",
              "      <td>0.391128</td>\n",
              "      <td>0.31796</td>\n",
              "      <td>0.32128</td>\n",
              "      <td>0.44467</td>\n",
              "      <td>0.327915</td>\n",
              "      <td>0.321570</td>\n",
              "      <td>0.605077</td>\n",
              "      <td>0.602642</td>\n",
              "      <td>939.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>...</td>\n",
              "      <td>H</td>\n",
              "      <td>D</td>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "      <td>A</td>\n",
              "      <td>P</td>\n",
              "      <td>F</td>\n",
              "      <td>J</td>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "      <td>E</td>\n",
              "      <td>K</td>\n",
              "      <td>G</td>\n",
              "      <td>B</td>\n",
              "      <td>H</td>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "      <td>Y</td>\n",
              "      <td>BM</td>\n",
              "      <td>A</td>\n",
              "      <td>K</td>\n",
              "      <td>CK</td>\n",
              "      <td>0.273204</td>\n",
              "      <td>0.159990</td>\n",
              "      <td>0.527991</td>\n",
              "      <td>0.473202</td>\n",
              "      <td>0.704268</td>\n",
              "      <td>0.178193</td>\n",
              "      <td>0.247408</td>\n",
              "      <td>0.24564</td>\n",
              "      <td>0.22089</td>\n",
              "      <td>0.21230</td>\n",
              "      <td>0.204687</td>\n",
              "      <td>0.202213</td>\n",
              "      <td>0.246011</td>\n",
              "      <td>0.432606</td>\n",
              "      <td>2763.85</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 132 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id cat1 cat2 cat3 cat4  ...    cont11    cont12    cont13    cont14     loss\n",
              "0   1    A    B    A    B  ...  0.569745  0.594646  0.822493  0.714843  2213.18\n",
              "1   2    A    B    A    A  ...  0.338312  0.366307  0.611431  0.304496  1283.60\n",
              "2   5    A    B    A    A  ...  0.381398  0.373424  0.195709  0.774425  3005.09\n",
              "3  10    B    B    A    B  ...  0.327915  0.321570  0.605077  0.602642   939.85\n",
              "4  11    A    B    A    B  ...  0.204687  0.202213  0.246011  0.432606  2763.85\n",
              "\n",
              "[5 rows x 132 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMo4tGtYmQXL",
        "outputId": "37851e01-f75f-45fc-f764-87a0fd6ed56b"
      },
      "source": [
        "train_df.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(188318, 132)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0PnTAnYnI5T",
        "outputId": "5f5a5e8e-c7f5-43b3-99d1-1d1de3a732a4"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "cat_cols = [x for x in train_df.columns if 'cat' in x]\n",
        "cont_cols = [x for x in train_df.columns if 'cont' in x]\n",
        "le_cat_encoder = LabelEncoder()\n",
        "# fit the encoder based on training and test datasets\n",
        "le_cat_encoder.fit(np.concatenate([train_df[x] for x in cat_cols]+\n",
        "                                 [test_df[x] for x in cat_cols]))\n",
        "print(len(le_cat_encoder.classes_), 'categories')\n",
        "y_col = 'loss'"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "350 categories\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNH5pov4nNHR",
        "outputId": "b3441c10-b14a-4d4d-f3b8-98ef3e49b9a5"
      },
      "source": [
        "all_emb_chan, all_inputs = [], []\n",
        "for k in cat_cols:\n",
        "    in_val = Input(shape = (1,), name = k)\n",
        "    all_emb_chan +=[Embedding(len(le_cat_encoder.classes_)+1, 64)(in_val)]\n",
        "    all_inputs += [in_val]\n",
        "concat_layer = concatenate(all_emb_chan, axis = 1) # concatenate all of the columns together\n",
        "\n",
        "norm_concat_emb = BatchNormalization()(concat_layer)\n",
        "feature_layer = TimeDistributed(Dense(32))(Dropout(0.5)(norm_concat_emb))\n",
        "feature_lstm = Bidirectional(LSTM(16))(feature_layer)\n",
        "\n",
        "cont_input = Input(shape = (len(cont_cols),), name = 'continuous')\n",
        "bn_cont = BatchNormalization()(cont_input)\n",
        "cont_feature_layer = Dense(16)(Dropout(0.5)(bn_cont))\n",
        "full_concat_layer = concatenate([feature_lstm, cont_feature_layer])\n",
        "full_reduction = Dense(16)(full_concat_layer)\n",
        "\n",
        "out_layer = Dense(1, activation = 'tanh')(full_reduction)\n",
        "full_model = Model(inputs = all_inputs+[cont_input], \n",
        "                   outputs = [out_layer], name = 'FullModel')\n",
        "full_model.compile(optimizer = 'adam', loss = 'mae')\n",
        "print('Using a model with:', full_model.count_params(), 'parameters, in', len(full_model.layers), 'layers')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using a model with: 2615529 parameters, in 244 layers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeLpdheGnR5F"
      },
      "source": [
        "y_vec = train_df[y_col].copy().values\n",
        "loss_mean, loss_std = y_vec.mean(), 3*y_vec.std()\n",
        "y_vec -= loss_mean\n",
        "y_vec /= loss_std\n",
        "train_df['loss_norm'] = y_vec.clip(-1,1)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8v8SoSHinVZo",
        "outputId": "f2dae9ee-ca65-4c63-95c1-5f3a394416da"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "t_split_df, v_split_df = train_test_split(train_df, \n",
        "                 test_size = 0.2,\n",
        "                 stratify = pd.qcut(train_df['loss'], 10),\n",
        "                                         random_state = 2017)\n",
        "print(t_split_df.shape, v_split_df.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150654, 133) (37664, 133)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSTLf-3xn-6K"
      },
      "source": [
        "def gen_samples(in_df, batch_size = None, loss_name = 'loss_norm'):\n",
        "    while True:\n",
        "        out_df = in_df if batch_size is None else in_df.sample(batch_size)\n",
        "        feed_dict = {c_name: le_cat_encoder.transform(out_df[c_name].values) for c_name in cat_cols}\n",
        "        feed_dict['continuous'] = out_df[cont_cols].values\n",
        "        yield feed_dict, out_df[loss_name].values"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZAA6yPknXsO"
      },
      "source": [
        "loss_history = []"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvQh-xIBnyjn",
        "outputId": "67c5101d-aa9b-4b61-b360-4f7de2289538"
      },
      "source": [
        "for i in range(100):\n",
        "    loss_history += [full_model.fit_generator(gen_samples(t_split_df, 32), \n",
        "                         steps_per_epoch = 250,\n",
        "                         epochs = 1,\n",
        "                         validation_data = next(gen_samples(v_split_df))\n",
        "                         )]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "250/250 [==============================] - 68s 207ms/step - loss: 0.2820 - val_loss: 0.1810\n",
            "250/250 [==============================] - 48s 194ms/step - loss: 0.1795 - val_loss: 0.1666\n",
            "250/250 [==============================] - 49s 195ms/step - loss: 0.1704 - val_loss: 0.1637\n",
            "250/250 [==============================] - 49s 198ms/step - loss: 0.1659 - val_loss: 0.1589\n",
            "250/250 [==============================] - 50s 200ms/step - loss: 0.1625 - val_loss: 0.1566\n",
            "250/250 [==============================] - 49s 196ms/step - loss: 0.1517 - val_loss: 0.1473\n",
            "250/250 [==============================] - 49s 197ms/step - loss: 0.1507 - val_loss: 0.1424\n",
            "250/250 [==============================] - 49s 196ms/step - loss: 0.1459 - val_loss: 0.1434\n",
            "250/250 [==============================] - 49s 194ms/step - loss: 0.1453 - val_loss: 0.1398\n",
            "250/250 [==============================] - 48s 192ms/step - loss: 0.1415 - val_loss: 0.1373\n",
            "250/250 [==============================] - 48s 194ms/step - loss: 0.1420 - val_loss: 0.1404\n",
            "250/250 [==============================] - 50s 201ms/step - loss: 0.1388 - val_loss: 0.1364\n",
            "250/250 [==============================] - 49s 197ms/step - loss: 0.1424 - val_loss: 0.1383\n",
            "250/250 [==============================] - 49s 198ms/step - loss: 0.1359 - val_loss: 0.1364\n",
            "250/250 [==============================] - 52s 207ms/step - loss: 0.1375 - val_loss: 0.1364\n",
            "250/250 [==============================] - 51s 203ms/step - loss: 0.1400 - val_loss: 0.1353\n",
            "250/250 [==============================] - 49s 196ms/step - loss: 0.1340 - val_loss: 0.1355\n",
            "250/250 [==============================] - 49s 196ms/step - loss: 0.1404 - val_loss: 0.1341\n",
            "250/250 [==============================] - 48s 193ms/step - loss: 0.1392 - val_loss: 0.1339\n",
            "250/250 [==============================] - 48s 194ms/step - loss: 0.1373 - val_loss: 0.1324\n",
            "250/250 [==============================] - 49s 197ms/step - loss: 0.1359 - val_loss: 0.1314\n",
            "250/250 [==============================] - 48s 193ms/step - loss: 0.1385 - val_loss: 0.1352\n",
            "250/250 [==============================] - 49s 197ms/step - loss: 0.1333 - val_loss: 0.1324\n",
            "250/250 [==============================] - 49s 195ms/step - loss: 0.1369 - val_loss: 0.1331\n",
            "250/250 [==============================] - 48s 194ms/step - loss: 0.1300 - val_loss: 0.1310\n",
            "250/250 [==============================] - 49s 195ms/step - loss: 0.1369 - val_loss: 0.1341\n",
            "250/250 [==============================] - 49s 198ms/step - loss: 0.1340 - val_loss: 0.1314\n",
            "250/250 [==============================] - 53s 213ms/step - loss: 0.1350 - val_loss: 0.1326\n",
            "250/250 [==============================] - 50s 202ms/step - loss: 0.1349 - val_loss: 0.1384\n",
            "250/250 [==============================] - 49s 195ms/step - loss: 0.1376 - val_loss: 0.1332\n",
            "250/250 [==============================] - 48s 194ms/step - loss: 0.1316 - val_loss: 0.1331\n",
            "250/250 [==============================] - 48s 194ms/step - loss: 0.1312 - val_loss: 0.1324\n",
            "250/250 [==============================] - 51s 203ms/step - loss: 0.1337 - val_loss: 0.1322\n",
            "250/250 [==============================] - 49s 196ms/step - loss: 0.1342 - val_loss: 0.1324\n",
            "250/250 [==============================] - 49s 194ms/step - loss: 0.1323 - val_loss: 0.1307\n",
            "250/250 [==============================] - 49s 196ms/step - loss: 0.1336 - val_loss: 0.1304\n",
            "250/250 [==============================] - 49s 195ms/step - loss: 0.1329 - val_loss: 0.1306\n",
            "250/250 [==============================] - 48s 194ms/step - loss: 0.1299 - val_loss: 0.1309\n",
            "250/250 [==============================] - 49s 198ms/step - loss: 0.1326 - val_loss: 0.1301\n",
            "250/250 [==============================] - 49s 196ms/step - loss: 0.1280 - val_loss: 0.1332\n",
            "250/250 [==============================] - 49s 197ms/step - loss: 0.1323 - val_loss: 0.1296\n",
            "250/250 [==============================] - 49s 196ms/step - loss: 0.1305 - val_loss: 0.1297\n",
            "250/250 [==============================] - 48s 194ms/step - loss: 0.1314 - val_loss: 0.1303\n",
            "250/250 [==============================] - 50s 198ms/step - loss: 0.1314 - val_loss: 0.1305\n",
            "250/250 [==============================] - 49s 196ms/step - loss: 0.1322 - val_loss: 0.1296\n",
            "250/250 [==============================] - 50s 199ms/step - loss: 0.1291 - val_loss: 0.1298\n",
            "250/250 [==============================] - 50s 198ms/step - loss: 0.1322 - val_loss: 0.1293\n",
            "250/250 [==============================] - 49s 195ms/step - loss: 0.1322 - val_loss: 0.1298\n",
            "250/250 [==============================] - 49s 195ms/step - loss: 0.1317 - val_loss: 0.1291\n",
            "250/250 [==============================] - 48s 193ms/step - loss: 0.1290 - val_loss: 0.1307\n",
            "250/250 [==============================] - 48s 192ms/step - loss: 0.1297 - val_loss: 0.1317\n",
            "250/250 [==============================] - 57s 227ms/step - loss: 0.1305 - val_loss: 0.1317\n",
            "250/250 [==============================] - 51s 205ms/step - loss: 0.1307 - val_loss: 0.1292\n",
            "250/250 [==============================] - 51s 202ms/step - loss: 0.1252 - val_loss: 0.1287\n",
            "250/250 [==============================] - 51s 204ms/step - loss: 0.1304 - val_loss: 0.1293\n",
            "250/250 [==============================] - 52s 207ms/step - loss: 0.1272 - val_loss: 0.1351\n",
            "250/250 [==============================] - 51s 205ms/step - loss: 0.1313 - val_loss: 0.1344\n",
            "250/250 [==============================] - 53s 214ms/step - loss: 0.1312 - val_loss: 0.1318\n",
            "250/250 [==============================] - 56s 224ms/step - loss: 0.1292 - val_loss: 0.1291\n",
            "250/250 [==============================] - 54s 216ms/step - loss: 0.1303 - val_loss: 0.1287\n",
            "250/250 [==============================] - 51s 204ms/step - loss: 0.1304 - val_loss: 0.1291\n",
            "250/250 [==============================] - 50s 202ms/step - loss: 0.1276 - val_loss: 0.1300\n",
            "250/250 [==============================] - 51s 205ms/step - loss: 0.1319 - val_loss: 0.1301\n",
            "250/250 [==============================] - 52s 207ms/step - loss: 0.1307 - val_loss: 0.1313\n",
            "250/250 [==============================] - 52s 209ms/step - loss: 0.1307 - val_loss: 0.1297\n",
            "250/250 [==============================] - 53s 211ms/step - loss: 0.1332 - val_loss: 0.1338\n",
            "250/250 [==============================] - 52s 210ms/step - loss: 0.1314 - val_loss: 0.1283\n",
            "250/250 [==============================] - 50s 202ms/step - loss: 0.1274 - val_loss: 0.1289\n",
            "250/250 [==============================] - 51s 204ms/step - loss: 0.1263 - val_loss: 0.1307\n",
            "250/250 [==============================] - 50s 201ms/step - loss: 0.1320 - val_loss: 0.1300\n",
            "250/250 [==============================] - 51s 205ms/step - loss: 0.1297 - val_loss: 0.1285\n",
            "250/250 [==============================] - 51s 204ms/step - loss: 0.1274 - val_loss: 0.1298\n",
            "250/250 [==============================] - 51s 204ms/step - loss: 0.1249 - val_loss: 0.1297\n",
            "250/250 [==============================] - 51s 205ms/step - loss: 0.1291 - val_loss: 0.1293\n",
            "250/250 [==============================] - 52s 207ms/step - loss: 0.1305 - val_loss: 0.1285\n",
            "250/250 [==============================] - 51s 205ms/step - loss: 0.1269 - val_loss: 0.1282\n",
            "250/250 [==============================] - 53s 211ms/step - loss: 0.1292 - val_loss: 0.1285\n",
            "250/250 [==============================] - 52s 209ms/step - loss: 0.1288 - val_loss: 0.1289\n",
            "250/250 [==============================] - 51s 205ms/step - loss: 0.1266 - val_loss: 0.1294\n",
            "250/250 [==============================] - 51s 204ms/step - loss: 0.1316 - val_loss: 0.1298\n",
            "250/250 [==============================] - 52s 207ms/step - loss: 0.1309 - val_loss: 0.1284\n",
            "250/250 [==============================] - 51s 206ms/step - loss: 0.1300 - val_loss: 0.1286\n",
            "250/250 [==============================] - 51s 206ms/step - loss: 0.1281 - val_loss: 0.1285\n",
            "250/250 [==============================] - 53s 212ms/step - loss: 0.1279 - val_loss: 0.1315\n",
            "250/250 [==============================] - 52s 207ms/step - loss: 0.1282 - val_loss: 0.1282\n",
            "250/250 [==============================] - 52s 209ms/step - loss: 0.1234 - val_loss: 0.1296\n",
            "250/250 [==============================] - 54s 216ms/step - loss: 0.1299 - val_loss: 0.1291\n",
            "250/250 [==============================] - 52s 210ms/step - loss: 0.1277 - val_loss: 0.1287\n",
            "250/250 [==============================] - 53s 211ms/step - loss: 0.1257 - val_loss: 0.1289\n",
            "250/250 [==============================] - 53s 211ms/step - loss: 0.1302 - val_loss: 0.1280\n",
            "250/250 [==============================] - 50s 202ms/step - loss: 0.1273 - val_loss: 0.1281\n",
            "250/250 [==============================] - 50s 200ms/step - loss: 0.1298 - val_loss: 0.1280\n",
            "250/250 [==============================] - 51s 202ms/step - loss: 0.1299 - val_loss: 0.1277\n",
            "250/250 [==============================] - 52s 210ms/step - loss: 0.1264 - val_loss: 0.1288\n",
            "250/250 [==============================] - 50s 201ms/step - loss: 0.1301 - val_loss: 0.1281\n",
            "250/250 [==============================] - 51s 203ms/step - loss: 0.1281 - val_loss: 0.1293\n",
            "250/250 [==============================] - 51s 205ms/step - loss: 0.1302 - val_loss: 0.1282\n",
            "250/250 [==============================] - 50s 202ms/step - loss: 0.1248 - val_loss: 0.1317\n",
            "250/250 [==============================] - 51s 203ms/step - loss: 0.1273 - val_loss: 0.1293\n",
            "250/250 [==============================] - 50s 202ms/step - loss: 0.1302 - val_loss: 0.1276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmlKGJrLoCn4",
        "outputId": "e445bec4-ba4b-4a47-9f46-6729c0acbc06"
      },
      "source": [
        "%%time\n",
        "valid_vars, valid_loss = next(gen_samples(v_split_df, loss_name = 'loss'))\n",
        "pred_loss = full_model.predict(valid_vars).ravel()*loss_std+loss_mean"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 36.2 s, sys: 864 ms, total: 37.1 s\n",
            "Wall time: 22.8 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "3yWNc3UWoGvD",
        "outputId": "f4305c44-11dd-4e2d-f949-1c27b308c40a"
      },
      "source": [
        "fig, ax1 = plt.subplots(1,1)\n",
        "ax1.hist(valid_loss-pred_loss)\n",
        "ax1.set_title('Loss Error: MAE-%2.2f' % (np.mean(np.abs(valid_loss-pred_loss))))\n",
        "ax1.set_xlabel('Actual - Predicted Loss')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Actual - Predicted Loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20zuSPJnoJiD"
      },
      "source": [
        ""
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_NVNrFPoNVX"
      },
      "source": [
        "Make Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_tjSk1NoTrO",
        "outputId": "b5b222c6-c859-418a-f835-0bdfc94caa2a"
      },
      "source": [
        "%%time\n",
        "test_vars, test_id = next(gen_samples(test_df, loss_name = 'id'))\n",
        "pred_test_loss = full_model.predict(test_vars, verbose = 1).ravel()*loss_std+loss_mean"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3924/3924 [==============================] - 117s 29ms/step\n",
            "CPU times: user 2min 40s, sys: 3.63 s, total: 2min 44s\n",
            "Wall time: 1min 59s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM4l2k4WoXOA"
      },
      "source": [
        "pd.DataFrame(dict(id = test_id, loss = pred_test_loss)).to_csv('/content/drive/MyDrive/Allstate Claims Severity/prediction.csv', index = False)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkr26Jihoouc"
      },
      "source": [
        ""
      ],
      "execution_count": 36,
      "outputs": []
    }
  ]
}